{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID with GAN generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycv5080/miniconda3/envs/cs449_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ycv5080/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ycv5080/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104impl8GPUTrace13gpuTraceStateE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.ao.quantization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Variable\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchfusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgan\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m \u001b[39mimport\u001b[39;00m DCGANDiscriminator\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m MarioDataset\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcustom\u001b[39;00m \u001b[39mimport\u001b[39;00m Generator\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchfusion/__init__.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlearners\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchfusion/learners/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlearners\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardLearner,TextClassifier,BaseLearner, BaseTextLearner,AbstractBaseLearner\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchfusion/learners/learners.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m ceil\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mopen\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PlotInput, visualize, get_model_summary,get_batch_size,clip_grads,save_model,load_model\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboardX\u001b[39;00m \u001b[39mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlr_scheduler\u001b[39;00m \u001b[39mimport\u001b[39;00m ReduceLROnPlateau\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchfusion/utils/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m VisdomLogger\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model_summary,clip_grads,get_batch_size,decode_imagenet,one_hot,FieldInput,adjust_learning_rate,visualize,PlotInput,load_image,download_file,extract_tar,extract_zip,save_model,load_model\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchfusion/utils/utils.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtarfile\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/models/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mswin_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmaxvit\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m detection, optical_flow, quantization, segmentation, video\n\u001b[1;32m     19\u001b[0m \u001b[39m# The Weights and WeightsEnum are developer-facing utils that we make public for\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# downstream libs like torchgeo https://github.com/pytorch/vision/issues/7094\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# TODO: we could / should document them publicly, but it's not clear where, as\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# they're not intended for end users.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model, get_model_builder, get_model_weights, get_weight, list_models, Weights, WeightsEnum\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/models/quantization/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgooglenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minception\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mshufflenetv2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/models/quantization/mobilenet.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenetv2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401, F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenetv3\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401, F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenetv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __all__ \u001b[39mas\u001b[39;00m mv2_all\n",
      "File \u001b[0;32m~/miniconda3/envs/cs449_project/lib/python3.9/site-packages/torchvision/models/quantization/mobilenetv2.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Optional, Union\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m \u001b[39mimport\u001b[39;00m DeQuantStub, QuantStub\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmobilenetv2\u001b[39;00m \u001b[39mimport\u001b[39;00m InvertedResidual, MobileNet_V2_Weights, MobileNetV2\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2dNormActivation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.ao.quantization'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torchfusion.gan.applications import DCGANDiscriminator\n",
    "\n",
    "from data_loader import MarioDataset\n",
    "from models.custom import Generator\n",
    "\n",
    "import csv\n",
    "\n",
    "from image_gen.asset_map import get_asset_map\n",
    "from image_gen.fixer import PipeFixer\n",
    "from image_gen.image_gen import GameImageGenerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from get_level import GetLevel as getLevel\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the matrices\n",
    "def preprocess_matrices(matrices):\n",
    "    # Normalize values to the range [0, 255]\n",
    "    normalized_matrices = (matrices - np.min(matrices)) / (np.max(matrices) - np.min(matrices))\n",
    "    normalized_matrices = normalized_matrices * 255\n",
    "    return normalized_matrices.astype(np.uint8)\n",
    "\n",
    "# Function to compute mean and covariance of features\n",
    "def compute_statistics(matrices):\n",
    "    # Flatten matrices into vectors\n",
    "    flattened_matrices = matrices.reshape((matrices.shape[0], -1))\n",
    "    # Compute mean and covariance\n",
    "    mean = np.mean(flattened_matrices, axis=0)\n",
    "    covariance = np.cov(flattened_matrices, rowvar=False)\n",
    "\n",
    "    return mean, covariance\n",
    "\n",
    "# Function to compute Fréchet distance\n",
    "def compute_frechet_distance(real_mean, real_cov, generated_mean, generated_cov):\n",
    "    epsilon = 1e-6  # Small constant to avoid numerical instability\n",
    "    sqrt_cov_product = sqrtm(real_cov.dot(generated_cov))\n",
    "    fid_score = np.linalg.norm(real_mean - generated_mean) + np.trace(real_cov + generated_cov - 2 * sqrt_cov_product)\n",
    "\n",
    "    return fid_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches for Real samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = MarioDataset()\n",
    "ref_idx = torch.randperm(len(org_data))\n",
    "prev_frame, curr_frame = (org_data[:].prev_frame, org_data[:].curr_frame)\n",
    "complete_frame = torch.cat((prev_frame,curr_frame),dim=3)\n",
    "complete_frame = torch.argmax(complete_frame, dim = 1)\n",
    "#complete_frame = torch.tensor(complete_frame,dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess real data\n",
    "complete_frame_np = complete_frame.detach().numpy()\n",
    "real_matrices = preprocess_matrices(complete_frame_np)\n",
    "# Compute statistics for real matrices\n",
    "real_mean, real_cov = compute_statistics(real_matrices)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches for generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_channels = [0,1,6,7]\n",
    "def Gen_sample(conditional_channels,ini_data=120):\n",
    "    dataset = MarioDataset()\n",
    "    netG = Generator(\n",
    "            latent_size=(len(conditional_channels) + 1, 14, 14), out_size=(13, 32, 32)\n",
    "        )\n",
    "    netG.load_state_dict(torch.load(\"./trained_models/netG_epoch_300000_0_32.pth\"))\n",
    "        # 300000\n",
    "    mario_map = get_asset_map(game=\"mario\")\n",
    "    gen = GameImageGenerator(asset_map=mario_map)\n",
    "    prev_frame, curr_frame = dataset[[ini_data]]\n",
    "    fixer = PipeFixer()\n",
    "\n",
    "    level_gen = getLevel(netG, gen, fixer, prev_frame, curr_frame, conditional_channels)\n",
    "    var = 1\n",
    "    #noise = np.rand((1, 1, 14, 14)).normal_(0, var)\n",
    "    noise = np.random.normal(0,var,size=(14,14))\n",
    "    level = level_gen.generate_frames(noise, var=var, frame_count=1) # generated matrix without padded\n",
    "    padded = torch.zeros(32,32)\n",
    "    padded[9:-9,2:-2] = torch.from_numpy(level)\n",
    "    level = padded\n",
    "    #level_finalize = torch.zeros(1,32,32)\n",
    "    #level_finalize[0,:,:] = level\n",
    "    #level = level_finalize\n",
    "    return level\n",
    "    # this is just for visualization\n",
    "    #level_gen.gen.save_gen_level(img_name=\"test_fuc_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of generated samples \n",
    "n_gen = 1000\n",
    "\n",
    "# generate samples and stack together\n",
    "level_gen = torch.zeros(n_gen,32,32)\n",
    "for i in range(n_gen):\n",
    "    level = Gen_sample(conditional_channels,ini_data=120)\n",
    "    level_gen[i,:,:] = level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess generated data\n",
    "generated_frame_np = level_gen.detach().numpy()\n",
    "generated_matrices = preprocess_matrices(generated_frame_np)\n",
    "# Compute statistics for real matrices\n",
    "gen_mean, gen_cov = compute_statistics(generated_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID score: (136664.82718125774-1.1180391749244107e-05j)\n"
     ]
    }
   ],
   "source": [
    "# Compute Fréchet distance\n",
    "fid_score = compute_frechet_distance(real_mean, real_cov, gen_mean, gen_cov)\n",
    "print(\"FID score:\", fid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs449_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
